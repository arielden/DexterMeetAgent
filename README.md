# DexterMeetAgent ğŸ¯

Sistema de transcripciÃ³n inteligente para Google Meet que identifica a un participante especÃ­fico, transcribe cuando habla y genera respuestas cortas usando LLM local.

## âœ¨ CaracterÃ­sticas

- **Captura de audio** desde monitor de PulseAudio/PipeWire
- **DiarizaciÃ³n de speakers** con pyannote.audio para identificar participantes
- **TranscripciÃ³n** en tiempo real con OpenAI Whisper
- **Respuestas inteligentes** generadas con Ollama (LLM local)
- **Mapeo manual** de speaker_id a nombre de participante
- **Buffer inteligente** con detecciÃ³n de fin de intervenciÃ³n
- **Sin almacenamiento** de historial (privacidad)

## ğŸ”§ Requisitos del Sistema

### Software
- **OS**: Linux (Ubuntu/Debian/Fedora/Arch)
- **Python**: 3.10+
- **Audio**: PulseAudio o PipeWire
- **GPU**: Opcional (CUDA para mejor rendimiento)

### Hardware Recomendado
- **RAM**: 8GB+ (16GB recomendado para modelos grandes)
- **CPU**: Moderna con soporte AVX
- **GPU**: NVIDIA con CUDA (opcional pero mejora rendimiento)

## ğŸ“¦ InstalaciÃ³n

### 1. Clonar el repositorio
```bash
git clone <repository-url>
cd DexterMeetAgent
```

### 2. Crear entorno virtual
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Instalar dependencias
```bash
pip install -r requirements.txt
```

### 5. Configurar RAG (Opcional pero recomendado)
DexterMeetAgent incluye soporte para **RAG (Retrieval-Augmented Generation)** que permite alimentar al LLM con informaciÃ³n especÃ­fica de tu dominio.

#### Indexar base de conocimientos
```bash
# Indexar un archivo de texto con informaciÃ³n tÃ©cnica
python index_knowledge_base.py knowledge_base.txt

# O indexar mÃºltiples archivos
python index_knowledge_base.py documento1.txt
python index_knowledge_base.py documento2.md
```

#### Archivo de ejemplo
Se incluye `knowledge_base.txt` con informaciÃ³n tÃ©cnica del sistema como ejemplo.

## ğŸ¤– ConfiguraciÃ³n de LLM

### Ollama Setup
1. **Instalar Ollama**:
   ```bash
   curl -fsSL https://ollama.ai/install.sh | sh
   ```

2. **Iniciar servidor**:
   ```bash
   ollama serve
   ```

3. **Descargar modelo** (recomendado: llama3.2:3b):
   ```bash
   ollama pull llama3.2:3b
   ```

### Modelos recomendados
- **llama3.2:3b**: Equilibrio velocidad/calidad
- **mistral:7b**: Mejor comprensiÃ³n contextual  
- **gemma:2b**: Respuestas muy cortas

### Variables de entorno (.env)
```bash
OLLAMA_MODEL=llama3.2:3b
OLLAMA_URL=http://localhost:11434
DEBUG=false
USE_RAG=true
```

## ğŸ§  Sistema RAG (Retrieval-Augmented Generation)

DexterMeetAgent utiliza **RAG** para proporcionar respuestas mÃ¡s precisas y contextuales basadas en tu base de conocimientos especÃ­fica.

### Â¿CÃ³mo funciona RAG?
1. **IndexaciÃ³n**: Tus documentos se dividen en fragmentos y se convierten en vectores
2. **BÃºsqueda**: Para cada pregunta, se buscan los fragmentos mÃ¡s relevantes
3. **GeneraciÃ³n**: El LLM recibe el contexto relevante junto con la pregunta

### Beneficios
- âœ… Respuestas basadas en conocimiento especÃ­fico de tu dominio
- âœ… ReducciÃ³n de alucinaciones del modelo
- âœ… FÃ¡cil actualizaciÃ³n de informaciÃ³n
- âœ… Privacidad (todo queda local)

### GestiÃ³n de Base de Conocimientos

#### Indexar documentos
```bash
# Archivo Ãºnico
python index_knowledge_base.py mi_documento.txt

# MÃºltiples archivos
python index_knowledge_base.py docs/tecnico.md
python index_knowledge_base.py docs/manual.pdf
```

#### Formatos soportados
- **Texto plano** (.txt)
- **Markdown** (.md)
- **PDF** (prÃ³ximamente)

#### Ver estadÃ­sticas
```python
from rag_client import rag_client
stats = rag_client.get_collection_stats()
print(stats)  # {'total_documents': 25, ...}
```

#### Limpiar base de conocimientos
```python
from rag_client import rag_client
rag_client.clear_collection()
```

### ConfiguraciÃ³n RAG
```python
# En config.py
@dataclass
class RAGConfig:
    collection_name: str = "knowledge_base"
    embedding_model: str = "all-MiniLM-L6-v2"
    chunk_size: int = 1000
    chunk_overlap: int = 200
    top_k: int = 3
```

## ğŸš€ Uso
```
sudo apt install portaudio19-dev python3-pyaudio pulseaudio-utils

# Fedora
sudo dnf install portaudio-devel python3-pyaudio pulseaudio-utils

# Arch Linux
sudo pacman -S portaudio python-pyaudio pulseaudio
```

### 5. Instalar y configurar Ollama
```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Iniciar servicio
ollama serve &

# Descargar modelo recomendado
ollama pull llama3.2:3b
# o para mayor calidad (mÃ¡s lento):
# ollama pull mistral:7b
```

### 6. Configurar HuggingFace Token
1. Crear cuenta en [HuggingFace](https://huggingface.co/)
2. Generar token en [Settings > Access Tokens](https://huggingface.co/settings/tokens)
3. Aceptar tÃ©rminos del modelo [pyannote/speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1)

### 7. Crear archivo de configuraciÃ³n
```bash
cp .env.example .env
# Editar .env con tu token de HuggingFace
```

Contenido de `.env`:
```env
HUGGINGFACE_TOKEN=hf_tu_token_aqui
OLLAMA_MODEL=llama3.2:3b
OLLAMA_URL=http://localhost:11434
DEBUG=false
```

## ğŸš€ Uso

### 1. Verificar configuraciÃ³n de audio
```bash
# Listar dispositivos de audio
python audio_capture.py

# Verificar monitor de PulseAudio
pactl list sources | grep -i monitor
```

### 2. Iniciar Google Meet
1. Abrir Google Meet en navegador
2. Unirse a reuniÃ³n
3. Asegurar que hay conversaciÃ³n activa

### 3. Ejecutar DexterMeetAgent
```bash
python main.py
```

### 4. ConfiguraciÃ³n inicial
1. El sistema capturarÃ¡ audio por 10 segundos
2. Se identificarÃ¡n automÃ¡ticamente los speakers
3. Elegir quÃ© speaker corresponde al participante objetivo
4. El sistema iniciarÃ¡ monitoreo continuo

### Ejemplo de ejecuciÃ³n:
```
ğŸ¯ DexterMeetAgent - TranscripciÃ³n Inteligente para Google Meet
============================================================

=== INICIALIZANDO DEXTERMEETAGENT ===
âœ“ Ollama configurado correctamente
âœ“ Whisper cargado correctamente
âœ“ Diarizador inicializado
âœ“ Dispositivo monitor: alsa_output.pci-0000_00_1f.3.analog-stereo.monitor

=== CONFIGURACIÃ“N DE PARTICIPANTE ===
Nombre del participante a monitorear: Juan PÃ©rez

Capturando audio inicial para identificar speakers...
Capturando por 10 segundos...

=== SPEAKERS DETECTADOS ===
1. Speaker SPEAKER_00
   Segmentos: 3
   DuraciÃ³n total: 12.4s
   Tiempos: 2.1-5.3s 8.7-12.1s 15.2-18.9s

2. Speaker SPEAKER_01
   Segmentos: 2
   DuraciÃ³n total: 8.1s
   Tiempos: 0.5-2.0s 13.5-19.1s

Â¿QuÃ© speaker corresponde a 'Juan PÃ©rez'? (1-2): 1
âœ“ SPEAKER_00 mapeado a 'Juan PÃ©rez'

=== INICIANDO MONITOREO ===
Presiona Ctrl+C para detener

============================================================
ğŸ‘¤ Juan PÃ©rez: Â¿Alguien puede explicar cÃ³mo funciona la integraciÃ³n continua?
ğŸ¤– Asistente: La integraciÃ³n continua es una prÃ¡ctica donde los desarrolladores integran cÃ³digo frecuentemente, ejecutando pruebas automÃ¡ticas para detectar errores temprano. Mejora la calidad del software y reduce conflictos.
============================================================
```

## âš™ï¸ ConfiguraciÃ³n

### Archivo `config.py`
Personalizar parÃ¡metros en `config.py`:

```python
# Audio
config.audio.sample_rate = 16000
config.audio.buffer_seconds = 5.0

# Modelos
config.transcription.model_size = "base"  # tiny, base, small, medium, large
config.llm.model_name = "llama3.2:3b"

# Prompt personalizado
config.llm.prompt_template = "Tu prompt personalizado: {transcription}"
```

### Variables de entorno (`.env`)
```env
HUGGINGFACE_TOKEN=hf_your_token_here
OLLAMA_MODEL=llama3.2:3b
OLLAMA_URL=http://localhost:11434
DEBUG=false
```

## ğŸ§ª Pruebas

### Probar componentes individualmente:
```bash
# Audio
python audio_capture.py

# DiarizaciÃ³n
python diarizer.py

# TranscripciÃ³n
python transcriber.py

# LLM
python llm_client.py
```

## ğŸ”§ SoluciÃ³n de Problemas

### Audio no se captura
```bash
# Verificar dispositivos PulseAudio
pactl list sources short

# Verificar permisos
groups $USER | grep audio

# Reiniciar PulseAudio
pulseaudio -k && pulseaudio --start
```

### Error de token HuggingFace
1. Verificar token en `.env`
2. Aceptar tÃ©rminos en [pyannote/speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1)

### Ollama no responde
```bash
# Verificar estado
curl http://localhost:11434/api/tags

# Reiniciar servicio
pkill ollama && ollama serve &

# Verificar modelo
ollama list
```

### Rendimiento lento
1. **GPU**: Instalar CUDA y PyTorch con soporte GPU
2. **Modelo Whisper**: Usar `tiny` o `base` en lugar de `large`
3. **Modelo Ollama**: Usar `llama3.2:3b` en lugar de modelos mÃ¡s grandes

## ğŸ“Š Rendimiento Esperado

### Latencia tÃ­pica:
- **Captura + DiarizaciÃ³n**: 2-4s
- **TranscripciÃ³n (Whisper base)**: 3-6s
- **GeneraciÃ³n LLM**: 3-7s
- **Total**: 8-17s

### PrecisiÃ³n:
- **DiarizaciÃ³n**: 85-95% (depende de calidad audio y nÃºmero de speakers)
- **TranscripciÃ³n**: 90-98% (espaÃ±ol, audio claro)
- **Relevancia respuestas**: Depende del modelo LLM

## ğŸ› ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Google Meet   â”‚â”€â”€â”€â”€â”‚  PulseAudio      â”‚â”€â”€â”€â”€â”‚  AudioCapture    â”‚
â”‚   (navegador)   â”‚    â”‚  Monitor         â”‚    â”‚  (pyaudio)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Ollama      â”‚â”€â”€â”€â”€â”‚    Main Loop     â”‚â”€â”€â”€â”€â”‚   SpeakerDiarizerâ”‚
â”‚   (LLM local)   â”‚    â”‚  (main.py)       â”‚    â”‚ (pyannote.audio) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                         â”‚
                                â–¼                         â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ WhisperTranscriberâ”‚    â”‚   VAD + Buffer   â”‚
                       â”‚ (openai-whisper) â”‚    â”‚  (webrtcvad)     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crear rama feature (`git checkout -b feature/AmazingFeature`)
3. Commit cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abrir Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ”’ Privacidad

- **No se almacena audio** ni transcripciones
- **Procesamiento local** (sin APIs cloud)
- **Token HuggingFace** solo para descarga de modelos
- **Ollama local** sin envÃ­o de datos externos

## ğŸ“š Referencias

- [OpenAI Whisper](https://github.com/openai/whisper)
- [pyannote.audio](https://github.com/pyannote/pyannote-audio)
- [Ollama](https://ollama.ai/)
- [PulseAudio](https://www.freedesktop.org/wiki/Software/PulseAudio/)

## ğŸ†˜ Soporte

Para reportar bugs o solicitar features, abrir un issue en GitHub.

---

**DexterMeetAgent** - TranscripciÃ³n inteligente para reuniones virtuales ğŸ¯