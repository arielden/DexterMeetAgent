# DexterMeetAgent ğŸ¯

Sistema de transcripciÃ³n inteligente para Google Meet que identifica a un participante especÃ­fico, transcribe cuando habla y genera respuestas cortas usando LLM local.

## âœ¨ CaracterÃ­sticas

- **Captura de audio** desde monitor de PulseAudio/PipeWire
- **DiarizaciÃ³n de speakers** con pyannote.audio para identificar participantes
- **TranscripciÃ³n** en tiempo real con OpenAI Whisper
- **Respuestas inteligentes** generadas con Ollama (LLM local)
- **Mapeo manual** de speaker_id a nombre de participante
- **Buffer inteligente** con detecciÃ³n de fin de intervenciÃ³n
- **Sin almacenamiento** de historial (privacidad)

## ğŸ”§ Requisitos del Sistema

### Software
- **OS**: Linux (Ubuntu/Debian/Fedora/Arch)
- **Python**: 3.10+
- **Audio**: PulseAudio o PipeWire
- **GPU**: Opcional (CUDA para mejor rendimiento)

### Hardware Recomendado
- **RAM**: 8GB+ (16GB recomendado para modelos grandes)
- **CPU**: Moderna con soporte AVX
- **GPU**: NVIDIA con CUDA (opcional pero mejora rendimiento)

## ğŸ“¦ InstalaciÃ³n

### 1. Clonar el repositorio
```bash
git clone <repository-url>
cd DexterMeetAgent
```

### 2. Crear entorno virtual
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Instalar dependencias
```bash
pip install -r requirements.txt
```

### 4. Instalar dependencias del sistema
```bash
# Ubuntu/Debian
sudo apt update
sudo apt install portaudio19-dev python3-pyaudio pulseaudio-utils

# Fedora
sudo dnf install portaudio-devel python3-pyaudio pulseaudio-utils

# Arch Linux
sudo pacman -S portaudio python-pyaudio pulseaudio
```

### 5. Instalar y configurar Ollama
```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Iniciar servicio
ollama serve &

# Descargar modelo recomendado
ollama pull llama3.2:3b
# o para mayor calidad (mÃ¡s lento):
# ollama pull mistral:7b
```

### 6. Configurar HuggingFace Token
1. Crear cuenta en [HuggingFace](https://huggingface.co/)
2. Generar token en [Settings > Access Tokens](https://huggingface.co/settings/tokens)
3. Aceptar tÃ©rminos del modelo [pyannote/speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1)

### 7. Crear archivo de configuraciÃ³n
```bash
cp .env.example .env
# Editar .env con tu token de HuggingFace
```

Contenido de `.env`:
```env
HUGGINGFACE_TOKEN=hf_tu_token_aqui
OLLAMA_MODEL=llama3.2:3b
OLLAMA_URL=http://localhost:11434
DEBUG=false
```

## ğŸš€ Uso

### 1. Verificar configuraciÃ³n de audio
```bash
# Listar dispositivos de audio
python audio_capture.py

# Verificar monitor de PulseAudio
pactl list sources | grep -i monitor
```

### 2. Iniciar Google Meet
1. Abrir Google Meet en navegador
2. Unirse a reuniÃ³n
3. Asegurar que hay conversaciÃ³n activa

### 3. Ejecutar DexterMeetAgent
```bash
python main.py
```

### 4. ConfiguraciÃ³n inicial
1. El sistema capturarÃ¡ audio por 10 segundos
2. Se identificarÃ¡n automÃ¡ticamente los speakers
3. Elegir quÃ© speaker corresponde al participante objetivo
4. El sistema iniciarÃ¡ monitoreo continuo

### Ejemplo de ejecuciÃ³n:
```
ğŸ¯ DexterMeetAgent - TranscripciÃ³n Inteligente para Google Meet
============================================================

=== INICIALIZANDO DEXTERMEETAGENT ===
âœ“ Ollama configurado correctamente
âœ“ Whisper cargado correctamente
âœ“ Diarizador inicializado
âœ“ Dispositivo monitor: alsa_output.pci-0000_00_1f.3.analog-stereo.monitor

=== CONFIGURACIÃ“N DE PARTICIPANTE ===
Nombre del participante a monitorear: Juan PÃ©rez

Capturando audio inicial para identificar speakers...
Capturando por 10 segundos...

=== SPEAKERS DETECTADOS ===
1. Speaker SPEAKER_00
   Segmentos: 3
   DuraciÃ³n total: 12.4s
   Tiempos: 2.1-5.3s 8.7-12.1s 15.2-18.9s

2. Speaker SPEAKER_01
   Segmentos: 2
   DuraciÃ³n total: 8.1s
   Tiempos: 0.5-2.0s 13.5-19.1s

Â¿QuÃ© speaker corresponde a 'Juan PÃ©rez'? (1-2): 1
âœ“ SPEAKER_00 mapeado a 'Juan PÃ©rez'

=== INICIANDO MONITOREO ===
Presiona Ctrl+C para detener

============================================================
ğŸ‘¤ Juan PÃ©rez: Â¿Alguien puede explicar cÃ³mo funciona la integraciÃ³n continua?
ğŸ¤– Asistente: La integraciÃ³n continua es una prÃ¡ctica donde los desarrolladores integran cÃ³digo frecuentemente, ejecutando pruebas automÃ¡ticas para detectar errores temprano. Mejora la calidad del software y reduce conflictos.
============================================================
```

## âš™ï¸ ConfiguraciÃ³n

### Archivo `config.py`
Personalizar parÃ¡metros en `config.py`:

```python
# Audio
config.audio.sample_rate = 16000
config.audio.buffer_seconds = 5.0

# Modelos
config.transcription.model_size = "base"  # tiny, base, small, medium, large
config.llm.model_name = "llama3.2:3b"

# Prompt personalizado
config.llm.prompt_template = "Tu prompt personalizado: {transcription}"
```

### Variables de entorno (`.env`)
```env
HUGGINGFACE_TOKEN=hf_your_token_here
OLLAMA_MODEL=llama3.2:3b
OLLAMA_URL=http://localhost:11434
DEBUG=false
```

## ğŸ§ª Pruebas

### Probar componentes individualmente:
```bash
# Audio
python audio_capture.py

# DiarizaciÃ³n
python diarizer.py

# TranscripciÃ³n
python transcriber.py

# LLM
python llm_client.py
```

## ğŸ”§ SoluciÃ³n de Problemas

### Audio no se captura
```bash
# Verificar dispositivos PulseAudio
pactl list sources short

# Verificar permisos
groups $USER | grep audio

# Reiniciar PulseAudio
pulseaudio -k && pulseaudio --start
```

### Error de token HuggingFace
1. Verificar token en `.env`
2. Aceptar tÃ©rminos en [pyannote/speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1)

### Ollama no responde
```bash
# Verificar estado
curl http://localhost:11434/api/tags

# Reiniciar servicio
pkill ollama && ollama serve &

# Verificar modelo
ollama list
```

### Rendimiento lento
1. **GPU**: Instalar CUDA y PyTorch con soporte GPU
2. **Modelo Whisper**: Usar `tiny` o `base` en lugar de `large`
3. **Modelo Ollama**: Usar `llama3.2:3b` en lugar de modelos mÃ¡s grandes

## ğŸ“Š Rendimiento Esperado

### Latencia tÃ­pica:
- **Captura + DiarizaciÃ³n**: 2-4s
- **TranscripciÃ³n (Whisper base)**: 3-6s
- **GeneraciÃ³n LLM**: 3-7s
- **Total**: 8-17s

### PrecisiÃ³n:
- **DiarizaciÃ³n**: 85-95% (depende de calidad audio y nÃºmero de speakers)
- **TranscripciÃ³n**: 90-98% (espaÃ±ol, audio claro)
- **Relevancia respuestas**: Depende del modelo LLM

## ğŸ› ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Google Meet   â”‚â”€â”€â”€â”€â”‚  PulseAudio      â”‚â”€â”€â”€â”€â”‚  AudioCapture    â”‚
â”‚   (navegador)   â”‚    â”‚  Monitor         â”‚    â”‚  (pyaudio)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Ollama      â”‚â”€â”€â”€â”€â”‚    Main Loop     â”‚â”€â”€â”€â”€â”‚   SpeakerDiarizerâ”‚
â”‚   (LLM local)   â”‚    â”‚  (main.py)       â”‚    â”‚ (pyannote.audio) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                         â”‚
                                â–¼                         â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ WhisperTranscriberâ”‚    â”‚   VAD + Buffer   â”‚
                       â”‚ (openai-whisper) â”‚    â”‚  (webrtcvad)     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crear rama feature (`git checkout -b feature/AmazingFeature`)
3. Commit cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abrir Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ”’ Privacidad

- **No se almacena audio** ni transcripciones
- **Procesamiento local** (sin APIs cloud)
- **Token HuggingFace** solo para descarga de modelos
- **Ollama local** sin envÃ­o de datos externos

## ğŸ“š Referencias

- [OpenAI Whisper](https://github.com/openai/whisper)
- [pyannote.audio](https://github.com/pyannote/pyannote-audio)
- [Ollama](https://ollama.ai/)
- [PulseAudio](https://www.freedesktop.org/wiki/Software/PulseAudio/)

## ğŸ†˜ Soporte

Para reportar bugs o solicitar features, abrir un issue en GitHub.

---

**DexterMeetAgent** - TranscripciÃ³n inteligente para reuniones virtuales ğŸ¯